import torch

# N is batch size; D_in is input dimension;
# H is hidden dimension; D_out is output dimension.
N, D_in, H, D_out = 64, 1000, 100, 10

dtype = torch.float
device = 'cuda' if torch.cuda.is_available() else 'cpu'
# N is batch size; D_in is input dimension;
# H is hidden dimension; D_out is output dimension. N, D_in, H, D_out = 64, 1000, 100, 10
# Create random input and output data
x = torch.randn(N, D_in, device=device, dtype=dtype)
y = torch.randn(N, D_out, device=device, dtype=dtype)
# Randomly initialize weights
w1 = torch.randn(D_in, H, device=device, dtype=dtype)
w2 = torch.randn(H, D_out, device=device, dtype=dtype)
learning_rate = 1e-6 
for t in range(500):
        # Forward pass: compute predicted y
        h = x.mm(w1)
        h_relu = h.clamp(min=0)
        y_pred = h_relu.mm(w2)
        # Compute and print loss
        loss = (y_pred - y).pow(2).sum().item() 
        print(t, loss)
        # Backprop to compute gradients of w1 and w2 with respect to loss
        grad_y_pred = 2.0 * (y_pred - y)
        grad_w2 = h_relu.t().mm(grad_y_pred)
        grad_h_relu = grad_y_pred.mm(w2.t())
        grad_h = grad_h_relu.clone()
        grad_h[h < 0] = 0
        grad_w1 = x.t().mm(grad_h)
        # Update weights using gradient descent
        w1 -= learning_rate * grad_w1
        w2 -= learning_rate * grad_w2

# ------------------------
# Demo: Python lists vs NumPy arrays
import numpy as np

print("Python list operations:")
a = [1,2,3]
b = [4,5,6]
print("a+b:", a + b)
try:
    print(a * b)
except TypeError:
    print("a*b has no meaning for Python lists")

print()
print("Numpy array operations:")
a = np.array([1,2,3])
b = np.array([4,5,6])
print("a+b:", a + b)
print("a*b:", a * b)

import numpy as np

# Define a 2D NumPy array (matrix)
a = np.array([[1, 2, 3],
              [4, 5, 6]])

print("a:")
print(a)

import numpy as np
from typing import Callable, List

Array_Function = Callable[[np.ndarray], np.ndarray]
Chain = List[Array_Function]

# sigmoid function
def sigmoid(x: np.ndarray) -> np.ndarray:
    '''
    Apply the sigmoid function to each element in the input ndarray.
    '''
    return 1 / (1 + np.exp(-x))

# numerical derivative
def deriv(func: Callable[[np.ndarray], np.ndarray],
          input_: np.ndarray,
          delta: float = 0.001) -> np.ndarray:
    '''
    Evaluates the derivative of a function "func" at every element in the "input_" array.
    '''
    return (func(input_ + delta) - func(input_ - delta)) / (2 * delta)

# chain rule for two functions
def chain_deriv_2(chain: Chain, input_range: np.ndarray) -> np.ndarray:
    '''
    Uses the chain rule to compute the derivative of two nested functions:
    (f2(f1(x)))' = f2'(f1(x)) * f1'(x)
    '''
    assert len(chain) == 2, "This function requires 'Chain' objects of length 2"
    assert input_range.ndim == 1, "Function requires a 1 dimensional ndarray as input_range"

    f1 = chain[0]
    f2 = chain[1]

    # f1(x)
    f1_of_x = f1(input_range)

    # df1/dx
    df1dx = deriv(f1, input_range)

    # df2/du, evaluated at u = f1(x)
    df2du = deriv(f2, f1_of_x)

    # Chain rule
    return df1dx * df2du

# --- Example functions ---
def square(x: np.ndarray) -> np.ndarray:
    return x**2

# Demo usage
if __name__ == "__main__":
    PLOT_RANGE = np.arange(-3, 3, 0.01)

    chain_1 = [square, sigmoid]   # sigmoid(square(x))
    chain_2 = [sigmoid, square]   # square(sigmoid(x))

    print("Derivative of sigmoid(square(x)) at x=1:", chain_deriv_2(chain_1, np.array([1.0])))
    print("Derivative of square(sigmoid(x)) at x=1:", chain_deriv_2(chain_2, np.array([1.0])))


# Sum along columns (axis=0)
print("a.sum(axis=0):", a.sum(axis=0))   # [5 7 9]

# Sum along rows (axis=1)
print("a.sum(axis=1):", a.sum(axis=1))   # [ 6 15]

a = np.array([[1,2,3],
                  [4,5,6]])
b = np.array([10,20,30]) print("a+b:\n", a+b)

import numpy as np
from typing import List, Callable

# A Function takes in an ndarray as an argument and produces an ndarray
Array_Function = Callable[[np.ndarray], np.ndarray]

# A Chain is a list of such functions
Chain = List[Array_Function]


def chain_length_2(chain: Chain, a: np.ndarray) -> np.ndarray:
    """
    Evaluates two functions in a row, in a Chain of length 2.
    """
    assert len(chain) == 2, "Length of input 'chain' should be 2"
    f1 = chain[0]
    f2 = chain[1]
    return f2(f1(a))


# Example usage
def square(x: np.ndarray) -> np.ndarray:
    return x ** 2

def increment(x: np.ndarray) -> np.ndarray:
    return x + 1

chain = [square, increment]
x = np.array([1, 2, 3])

print("Input:", x)
print("Output:", chain_length_2(chain, x))

from typing import Callable
import numpy as np

# A Function takes in an ndarray as an argument and produces an ndarray
Array_Function = Callable[[np.ndarray], np.ndarray]

def multiple_inputs_add(x: np.ndarray, y: np.ndarray,
                        sigma: Array_Function) -> np.ndarray:
    '''
    Function with multiple inputs and addition, forward pass.
    '''
    assert x.shape == y.shape, "x and y must have the same shape"
    a = x + y
    return sigma(a)

def matmul_forward(X: ndarray, W: ndarray) -> ndarray:
    '''
    Computes the forward pass of a matrix multiplication.
    '''
    assert X.shape[1] == W.shape[0], \
    '''
    For matrix multiplication, the number of columns in the first array should match the number of rows in the second; instead the number of columns in the first array is {0} and the number of rows in the second array is {1}.
    '''.format(X.shape[1], W.shape[0])
    
    # matrix multiplication
    N = np.dot(X, W)
    return N

def matmul_backward_first(X: ndarray,
W: ndarray) -> ndarray:
        '''
        Computes the backward pass of a matrix multiplication with respect to the
        first argument.
        '''
        # backward pass
dNdX = np.transpose(W, (1, 0)) return dNdX

def matrix_forward_extra(X: np.ndarray, W: np.ndarray,
                         sigma: Array_Function) -> np.ndarray:
    """
    Computes the forward pass of a function involving matrix multiplication, then an extra function.
    """
    assert X.shape[1] == W.shape[0], (
        f"For matmul, X.shape[1] ({X.shape[1]}) must equal W.shape[0] ({W.shape[0]})."
    )
    # matrix multiplication
    N = np.dot(X, W)
    # feed the output through sigma
    S = sigma(N)
    return S

import numpy as np
from typing import Callable
Array_Function = Callable[[np.ndarray], np.ndarray]

def deriv(func: Array_Function, input_: np.ndarray, delta: float = 1e-3) -> np.ndarray:
    return (func(input_ + delta) - func(input_ - delta)) / (2 * delta)

def matrix_function_backward_1(X: np.ndarray, W: np.ndarray,
                               sigma: Array_Function) -> np.ndarray:
    """
    Computes the gradient dL/dX for L = sum(S), where:
      N = X @ W
      S = sigma(N)
    """
    assert X.shape[1] == W.shape[0], "Shape mismatch: X@(W) requires X.shape[1] == W.shape[0]"
    N = X @ W
    # dS/dN = sigma'(N)
    dSdN = deriv(sigma, N)              # same shape as N
    # dL/dX = dSdN @ W^T  (since dL/dS = 1 for L = sum(S))
    dLdX = dSdN @ W.T
    return dLdX

import numpy as np
from typing import Callable

Array_Function = Callable[[np.ndarray], np.ndarray]

def matrix_function_forward_sum(X: np.ndarray, W: np.ndarray,
                                sigma: Array_Function) -> float:
    """
    Forward pass:
      N = X @ W
      S = sigma(N)
      L = sum(S)
    Returns scalar L.
    """
    assert X.shape[1] == W.shape[0], (
        f"Shape mismatch: X.shape={X.shape}, W.shape={W.shape} "
        f"requires X.shape[1] == W.shape[0]."
    )
    N = np.dot(X, W)
    S = sigma(N)
    L = np.sum(S)
    return float(L)

import numpy as np
from typing import Callable

Array_Function = Callable[[np.ndarray], np.ndarray]

def deriv(func: Array_Function, input_: np.ndarray, delta: float = 1e-3) -> np.ndarray:
    """Numerical derivative of `func` at each element of `input_`."""
    return (func(input_ + delta) - func(input_ - delta)) / (2 * delta)

def sigmoid(z: np.ndarray) -> np.ndarray:
    return 1.0 / (1.0 + np.exp(-z))

def matrix_function_forward_sum(X: np.ndarray, W: np.ndarray,
                                sigma: Array_Function) -> float:
    """
    Forward pass:
      N = X @ W
      S = sigma(N)
      L = sum(S)
    Returns scalar L.
    """
    assert X.shape[1] == W.shape[0], (
        f"Shape mismatch: X.shape={X.shape}, W.shape={W.shape} "
        f"requires X.shape[1] == W.shape[0]."
    )
    N = X @ W
    S = sigma(N)
    L = float(np.sum(S))
    return L

def matrix_function_backward_sum_1(X: np.ndarray, W: np.ndarray,
                                   sigma: Array_Function) -> np.ndarray:
    """
    Backward pass for L = sum( sigma(X @ W) ) w.r.t. X.
    Returns dL/dX with the same shape as X.
    """
    assert X.shape[1] == W.shape[0], "Shape mismatch for X @ W."
    N = X @ W                          # (m, p)
    S = sigma(N)                       # (m, p)

    # dL/dS = 1 for L = sum(S)
    dLdS = np.ones_like(S)             # (m, p)
    dSdN = deriv(sigma, N)             # (m, p)  elementwise σ'(N)
    dLdN = dLdS * dSdN                 # (m, p)

    # For N = X @ W, dL/dX = dL/dN @ W^T
    dLdX = dLdN @ W.T                  # (m, n)
    return dLdX

# (Optional) gradient w.r.t. W:
def matrix_function_backward_sum_w(X: np.ndarray, W: np.ndarray,
                                   sigma: Array_Function) -> np.ndarray:
    """
    Backward pass for L = sum( sigma(X @ W) ) w.r.t. W.
    Returns dL/dW with the same shape as W.
    """
    N = X @ W
    dSdN = deriv(sigma, N)             # (m, p)
    # For N = X @ W, dL/dW = X^T @ dL/dN
    dLdW = X.T @ dSdN                  # (n, p)
    return dLdW

# --- Quick sanity check (numbers will vary slightly due to numerical derivative) ---
if __name__ == "__main__":
    np.random.seed(190204)
    X = np.random.randn(3, 3)
    W = np.random.randn(3, 2)

    print("L:", round(matrix_function_forward_sum(X, W, sigmoid), 4))
    print("dLdX:\n", matrix_function_backward_sum_1(X, W, sigmoid))
    print("dLdW:\n", matrix_function_backward_sum_w(X, W, sigmoid))
import numpy as np
from typing import Callable

Array_Function = Callable[[np.ndarray], np.ndarray]

def deriv(func: Array_Function, input_: np.ndarray, delta: float = 1e-3) -> np.ndarray:
    """Numerical derivative of `func` at each element of `input_`."""
    return (func(input_ + delta) - func(input_ - delta)) / (2 * delta)

def sigmoid(z: np.ndarray) -> np.ndarray:
    return 1.0 / (1.0 + np.exp(-z))

def matrix_function_forward_sum(X: np.ndarray, W: np.ndarray,
                                sigma: Array_Function) -> float:
    """
    Forward pass:
      N = X @ W
      S = sigma(N)
      L = sum(S)
    Returns scalar L.
    """
    assert X.shape[1] == W.shape[0], (
        f"Shape mismatch: X.shape={X.shape}, W.shape={W.shape} "
        f"requires X.shape[1] == W.shape[0]."
    )
    N = X @ W
    S = sigma(N)
    L = float(np.sum(S))
    return L

def matrix_function_backward_sum_1(X: np.ndarray, W: np.ndarray,
                                   sigma: Array_Function) -> np.ndarray:
    """
    Backward pass for L = sum( sigma(X @ W) ) w.r.t. X.
    Returns dL/dX with the same shape as X.
    """
    assert X.shape[1] == W.shape[0], "Shape mismatch for X @ W."
    N = X @ W                          # (m, p)
    S = sigma(N)                       # (m, p)

    # dL/dS = 1 for L = sum(S)
    dLdS = np.ones_like(S)             # (m, p)
    dSdN = deriv(sigma, N)             # (m, p)  elementwise σ'(N)
    dLdN = dLdS * dSdN                 # (m, p)

    # For N = X @ W, dL/dX = dL/dN @ W^T
    dLdX = dLdN @ W.T                  # (m, n)
    return dLdX

# (Optional) gradient w.r.t. W:
def matrix_function_backward_sum_w(X: np.ndarray, W: np.ndarray,
                                   sigma: Array_Function) -> np.ndarray:
    """
    Backward pass for L = sum( sigma(X @ W) ) w.r.t. W.
    Returns dL/dW with the same shape as W.
    """
    N = X @ W
    dSdN = deriv(sigma, N)             # (m, p)
    # For N = X @ W, dL/dW = X^T @ dL/dN
    dLdW = X.T @ dSdN                  # (n, p)
    return dLdW

# --- Quick sanity check (numbers will vary slightly due to numerical derivative) ---
if __name__ == "__main__":
    np.random.seed(190204)
    X = np.random.randn(3, 3)
    W = np.random.randn(3, 2)

    print("L:", round(matrix_function_forward_sum(X, W, sigmoid), 4))
    print("dLdX:\n", matrix_function_backward_sum_1(X, W, sigmoid))
    print("dLdW:\n", matrix_function_backward_sum_w(X, W, sigmoid))











