import torch

# N is batch size; D_in is input dimension;
# H is hidden dimension; D_out is output dimension.
N, D_in, H, D_out = 64, 1000, 100, 10

dtype = torch.float
device = 'cuda' if torch.cuda.is_available() else 'cpu'
# N is batch size; D_in is input dimension;
# H is hidden dimension; D_out is output dimension. N, D_in, H, D_out = 64, 1000, 100, 10
# Create random input and output data
x = torch.randn(N, D_in, device=device, dtype=dtype)
y = torch.randn(N, D_out, device=device, dtype=dtype)
# Randomly initialize weights
w1 = torch.randn(D_in, H, device=device, dtype=dtype)
w2 = torch.randn(H, D_out, device=device, dtype=dtype)
learning_rate = 1e-6 
for t in range(500):
        # Forward pass: compute predicted y
        h = x.mm(w1)
        h_relu = h.clamp(min=0)
        y_pred = h_relu.mm(w2)
        # Compute and print loss
        loss = (y_pred - y).pow(2).sum().item() 
        print(t, loss)
        # Backprop to compute gradients of w1 and w2 with respect to loss
        grad_y_pred = 2.0 * (y_pred - y)
        grad_w2 = h_relu.t().mm(grad_y_pred)
        grad_h_relu = grad_y_pred.mm(w2.t())
        grad_h = grad_h_relu.clone()
        grad_h[h < 0] = 0
        grad_w1 = x.t().mm(grad_h)
        # Update weights using gradient descent
        w1 -= learning_rate * grad_w1
        w2 -= learning_rate * grad_w2

# ------------------------
# Demo: Python lists vs NumPy arrays
import numpy as np

print("Python list operations:")
a = [1,2,3]
b = [4,5,6]
print("a+b:", a + b)
try:
    print(a * b)
except TypeError:
    print("a*b has no meaning for Python lists")

print()
print("Numpy array operations:")
a = np.array([1,2,3])
b = np.array([4,5,6])
print("a+b:", a + b)
print("a*b:", a * b)

import numpy as np

# Define a 2D NumPy array (matrix)
a = np.array([[1, 2, 3],
              [4, 5, 6]])

print("a:")
print(a)

import numpy as np
from typing import Callable, List

Array_Function = Callable[[np.ndarray], np.ndarray]
Chain = List[Array_Function]

# sigmoid function
def sigmoid(x: np.ndarray) -> np.ndarray:
    '''
    Apply the sigmoid function to each element in the input ndarray.
    '''
    return 1 / (1 + np.exp(-x))

# numerical derivative
def deriv(func: Callable[[np.ndarray], np.ndarray],
          input_: np.ndarray,
          delta: float = 0.001) -> np.ndarray:
    '''
    Evaluates the derivative of a function "func" at every element in the "input_" array.
    '''
    return (func(input_ + delta) - func(input_ - delta)) / (2 * delta)

# chain rule for two functions
def chain_deriv_2(chain: Chain, input_range: np.ndarray) -> np.ndarray:
    '''
    Uses the chain rule to compute the derivative of two nested functions:
    (f2(f1(x)))' = f2'(f1(x)) * f1'(x)
    '''
    assert len(chain) == 2, "This function requires 'Chain' objects of length 2"
    assert input_range.ndim == 1, "Function requires a 1 dimensional ndarray as input_range"

    f1 = chain[0]
    f2 = chain[1]

    # f1(x)
    f1_of_x = f1(input_range)

    # df1/dx
    df1dx = deriv(f1, input_range)

    # df2/du, evaluated at u = f1(x)
    df2du = deriv(f2, f1_of_x)

    # Chain rule
    return df1dx * df2du

# --- Example functions ---
def square(x: np.ndarray) -> np.ndarray:
    return x**2

# Demo usage
if __name__ == "__main__":
    PLOT_RANGE = np.arange(-3, 3, 0.01)

    chain_1 = [square, sigmoid]   # sigmoid(square(x))
    chain_2 = [sigmoid, square]   # square(sigmoid(x))

    print("Derivative of sigmoid(square(x)) at x=1:", chain_deriv_2(chain_1, np.array([1.0])))
    print("Derivative of square(sigmoid(x)) at x=1:", chain_deriv_2(chain_2, np.array([1.0])))


# Sum along columns (axis=0)
print("a.sum(axis=0):", a.sum(axis=0))   # [5 7 9]

# Sum along rows (axis=1)
print("a.sum(axis=1):", a.sum(axis=1))   # [ 6 15]

a = np.array([[1,2,3],
                  [4,5,6]])
b = np.array([10,20,30]) print("a+b:\n", a+b)

import numpy as np
from typing import List, Callable

# A Function takes in an ndarray as an argument and produces an ndarray
Array_Function = Callable[[np.ndarray], np.ndarray]

# A Chain is a list of such functions
Chain = List[Array_Function]


def chain_length_2(chain: Chain, a: np.ndarray) -> np.ndarray:
    """
    Evaluates two functions in a row, in a Chain of length 2.
    """
    assert len(chain) == 2, "Length of input 'chain' should be 2"
    f1 = chain[0]
    f2 = chain[1]
    return f2(f1(a))


# Example usage
def square(x: np.ndarray) -> np.ndarray:
    return x ** 2

def increment(x: np.ndarray) -> np.ndarray:
    return x + 1

chain = [square, increment]
x = np.array([1, 2, 3])

print("Input:", x)
print("Output:", chain_length_2(chain, x))

from typing import Callable
import numpy as np

# A Function takes in an ndarray as an argument and produces an ndarray
Array_Function = Callable[[np.ndarray], np.ndarray]

def multiple_inputs_add(x: np.ndarray, y: np.ndarray,
                        sigma: Array_Function) -> np.ndarray:
    '''
    Function with multiple inputs and addition, forward pass.
    '''
    assert x.shape == y.shape, "x and y must have the same shape"
    a = x + y
    return sigma(a)

def matmul_forward(X: ndarray, W: ndarray) -> ndarray:
    '''
    Computes the forward pass of a matrix multiplication.
    '''
    assert X.shape[1] == W.shape[0], \
    '''
    For matrix multiplication, the number of columns in the first array should match the number of rows in the second; instead the number of columns in the first array is {0} and the number of rows in the second array is {1}.
    '''.format(X.shape[1], W.shape[0])
    
    # matrix multiplication
    N = np.dot(X, W)
    return N

def matmul_backward_first(X: ndarray,
W: ndarray) -> ndarray:
        '''
        Computes the backward pass of a matrix multiplication with respect to the
        first argument.
        '''
        # backward pass
dNdX = np.transpose(W, (1, 0)) return dNdX




